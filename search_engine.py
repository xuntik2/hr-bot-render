"""
–ü–û–ò–°–ö–û–í–´–ô –î–í–ò–ñ–û–ö –î–õ–Ø HR-–ë–û–¢–ê –ú–ï–ß–ï–õ
–í–µ—Ä—Å–∏—è 4.5 ‚Äî –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ (–±—ã—Å—Ç—Ä–∞—è –ø—Ä–µ–¥—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è + –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω),
–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é –∑–∞–ø—Ä–æ—Å–∞.
"""

import logging
import json
import os
import re
import hashlib
from typing import List, Optional, Tuple, Dict, Any
from dataclasses import dataclass
from collections import OrderedDict, defaultdict
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

# ------------------------------------------------------------
#  –§–£–ù–ö–¶–ò–Ø –õ–ï–í–ï–ù–®–¢–ï–ô–ù–ê
# ------------------------------------------------------------
def levenshtein_distance(s1: str, s2: str) -> int:
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ –º–µ–∂–¥—É –¥–≤—É–º—è —Å—Ç—Ä–æ–∫–∞–º–∏."""
    if len(s1) < len(s2):
        return levenshtein_distance(s2, s1)
    if len(s2) == 0:
        return len(s1)
    previous_row = range(len(s2) + 1)
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row
    return previous_row[-1]

@dataclass
class FAQEntry:
    id: int
    question: str
    answer: str
    keywords: str
    norm_keywords: str
    norm_question: str
    category: str
    usage_count: int = 0

class SearchEngine:
    """
    –ü–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –Ω–µ—á—ë—Ç–∫–∏–º –ø–æ–∏—Å–∫–æ–º:
    1. –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è (–±–µ–∑ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞).
    2. –¢–æ–ª—å–∫–æ –¥–ª—è —Ç–æ–ø-10 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ ‚Äî —Ä–∞—Å—á—ë—Ç –ø–æ–ª–Ω–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Å –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–æ–º.
    3. –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –±–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
    """
    
    STOP_WORDS = {
        '–∫–∞–∫', '—á—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '—Å–∫–æ–ª—å–∫–æ', '—á–µ–π',
        '–∞', '–∏', '–Ω–æ', '–∏–ª–∏', '–µ—Å–ª–∏', '—Ç–æ', '–∂–µ', '–±—ã', '–≤', '–Ω–∞', '—Å', '–ø–æ',
        '–æ', '–æ–±', '–æ—Ç', '–¥–æ', '–¥–ª—è', '–∏–∑', '—É', '–Ω–µ', '–Ω–µ—Ç', '–¥–∞', '—ç—Ç–æ',
        '—Ç–æ—Ç', '—ç—Ç–æ—Ç', '—Ç–∞–∫–æ–π', '–∫–∞–∫–æ–π', '–≤—Å–µ', '–≤—Å—ë', '–µ–≥–æ', '–µ–µ', '–∏—Ö',
        '–º–æ–∂–Ω–æ', '–Ω—É–∂–Ω–æ', '–Ω–∞–¥–æ', '–±—É–¥–µ—Ç', '–µ—Å—Ç—å', '–±—ã—Ç—å', '–≤–µ—Å—å', '—ç—Ç–∞', '—ç—Ç–∏'
    }
    
    SYNONYMS = {
        '–∑–ø': '–∑–∞—Ä–ø–ª–∞—Ç–∞',
        '–æ—Ç–¥—ã—Ö': '–æ—Ç–ø—É—Å–∫',
        '–±–æ–ª—å–Ω–∏—á–Ω—ã–π': '–ª–∏—Å—Ç–æ–∫ –Ω–µ—Ç—Ä—É–¥–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏',
        '–¥–µ–∫—Ä–µ—Ç': '–æ—Ç–ø—É—Å–∫ –ø–æ —É—Ö–æ–¥—É –∑–∞ —Ä–µ–±–µ–Ω–∫–æ–º',
        '—É–≤–æ–ª—å–Ω–µ–Ω–∏–µ': '—Ä–∞—Å—á–µ—Ç',
        '–ø—Ä–µ–º–∏—è': '–±–æ–Ω—É—Å',
        '—Å–ø—Ä–∞–≤–∫–∞': '–¥–æ–∫—É–º–µ–Ω—Ç',
        '—Ç—Ä—É–¥–æ–≤–∞—è': '—Ç—Ä—É–¥–æ–≤–∞—è –∫–Ω–∏–∂–∫–∞',
        '–æ–∫–ª–∞–¥': '–∑–∞—Ä–ø–ª–∞—Ç–∞',
        '–æ—Ç–≥—É–ª': '–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤—ã—Ö–æ–¥–Ω–æ–π',
        '–∫–∞–¥—Ä—ã': '–æ—Ç–¥–µ–ª –∫–∞–¥—Ä–æ–≤',
        '–ª—å–≥–æ—Ç–∞': '—Å–æ—Ü–∏–∞–ª—å–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞'
    }

    def __init__(self, max_cache_size: int = 200):
        self.max_cache_size = max_cache_size
        self.cache = OrderedDict()
        self.cache_ttl = {}
        self.faq_data: List[FAQEntry] = []
        self._category_index: Dict[str, List[FAQEntry]] = defaultdict(list)
        
        self.stats = {
            'total_searches': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'loaded_from': '–Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ'
        }
        
        self._load_faq()
        self._build_category_index()
        logger.info(f"‚úÖ SearchEngine v4.5: –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.faq_data)} –∑–∞–ø–∏—Å–µ–π, "
                   f"–∏—Å—Ç–æ—á–Ω–∏–∫: {self.stats['loaded_from']}, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–µ—á—ë—Ç–∫–∏–π –ø–æ–∏—Å–∫")

    # ------------------------------------------------------------
    #  –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•
    # ------------------------------------------------------------
    def _load_faq(self):
        if self._load_from_json():
            return
        logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å faq.json, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã")
        self._load_fallback()

    def _load_from_json(self) -> bool:
        json_path = "faq.json"
        if not os.path.exists(json_path):
            logger.debug(f"–§–∞–π–ª {json_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return False
        
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            self.faq_data.clear()
            loaded_count = 0
            for idx, item in enumerate(data, start=1):
                question = item.get('question', '').strip()
                answer = item.get('answer', '').strip()
                if not question or not answer:
                    continue
                
                keywords_raw = item.get('keywords', '')
                if isinstance(keywords_raw, list):
                    keywords_str = ', '.join(keywords_raw)
                else:
                    keywords_str = keywords_raw
                
                norm_keywords = item.get('norm_keywords', '')
                if not norm_keywords and keywords_str:
                    norm_keywords = self._normalize_text(keywords_str)
                
                norm_question = item.get('norm_question', '')
                if not norm_question and question:
                    norm_question = self._normalize_text(question)
                
                faq = FAQEntry(
                    id=idx,
                    question=question,
                    answer=answer,
                    keywords=keywords_str,
                    norm_keywords=norm_keywords,
                    norm_question=norm_question,
                    category=item.get('category', '–ë–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏').strip()
                )
                self.faq_data.append(faq)
                loaded_count += 1
            
            self.stats['loaded_from'] = f'JSON ({loaded_count} –∑–∞–ø–∏—Å–µ–π)'
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {loaded_count} –∑–∞–ø–∏—Å–µ–π –∏–∑ {json_path}")
            return True
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ JSON: {e}")
            return False

    def _load_fallback(self):
        self.faq_data = [
            FAQEntry(
                id=1,
                question="–ö–∞–∫ –æ—Ñ–æ—Ä–º–∏—Ç—å –æ—Ç–ø—É—Å–∫?",
                answer="–û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –≤ –æ—Ç–¥–µ–ª –∫–∞–¥—Ä–æ–≤ —Å –∑–∞—è–≤–ª–µ–Ω–∏–µ–º –∑–∞ 2 –Ω–µ–¥–µ–ª–∏ –¥–æ –Ω–∞—á–∞–ª–∞ –æ—Ç–ø—É—Å–∫–∞.",
                keywords="–æ—Ç–ø—É—Å–∫, –æ—Ñ–æ—Ä–º–∏—Ç—å, –∫–∞–¥—Ä—ã, –∑–∞—è–≤–ª–µ–Ω–∏–µ",
                norm_keywords="–æ—Ç–ø—É—Å–∫ –æ—Ñ–æ—Ä–º–∏—Ç—å –∫–∞–¥—Ä—ã –∑–∞—è–≤–ª–µ–Ω–∏–µ",
                norm_question="–∫–∞–∫ –æ—Ñ–æ—Ä–º–∏—Ç—å –æ—Ç–ø—É—Å–∫",
                category="–û—Ç–ø—É—Å–∫"
            ),
            FAQEntry(
                id=2,
                question="–ö–æ–≥–¥–∞ –≤—ã–ø–ª–∞—á–∏–≤–∞–µ—Ç—Å—è –∑–∞—Ä–ø–ª–∞—Ç–∞?",
                answer="–ó–∞—Ä–ø–ª–∞—Ç–∞ –≤—ã–ø–ª–∞—á–∏–≤–∞–µ—Ç—Å—è 5 –∏ 20 —á–∏—Å–ª–∞ –∫–∞–∂–¥–æ–≥–æ –º–µ—Å—è—Ü–∞.",
                keywords="–∑–∞—Ä–ø–ª–∞—Ç–∞, –≤—ã–ø–ª–∞—Ç–∞, –¥–∞—Ç–∞, –∞–≤–∞–Ω—Å",
                norm_keywords="–∑–∞—Ä–ø–ª–∞—Ç–∞ –≤—ã–ø–ª–∞—Ç–∞ –¥–∞—Ç–∞ –∞–≤–∞–Ω—Å",
                norm_question="–∫–æ–≥–¥–∞ –≤—ã–ø–ª–∞—á–∏–≤–∞–µ—Ç—Å—è –∑–∞—Ä–ø–ª–∞—Ç–∞",
                category="–ó–∞—Ä–ø–ª–∞—Ç–∞"
            )
        ]
        self.stats['loaded_from'] = '—Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (2 –∑–∞–ø–∏—Å–∏)'

    def _build_category_index(self):
        self._category_index.clear()
        for faq in self.faq_data:
            cat_lower = faq.category.lower()
            self._category_index[cat_lower].append(faq)

    # ------------------------------------------------------------
    #  –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê
    # ------------------------------------------------------------
    def _normalize_text(self, text: str) -> str:
        if not text:
            return ""
        
        text = text.lower().strip()
        for orig, repl in self.SYNONYMS.items():
            text = re.sub(r'\b' + re.escape(orig) + r'\b', repl, text)
        
        text = re.sub(r'[^\w\s]', ' ', text)
        words = text.split()
        words = [w for w in words if w not in self.STOP_WORDS and len(w) > 2]
        
        normalized = []
        for w in words:
            if w.endswith('—Ç—å—Å—è'): w = w[:-4] + '—Ç—å'
            elif w.endswith('—Ç—Å—è'): w = w[:-3] + '—Ç—å—Å—è'
            elif w.endswith('–∞—Ç—å') and len(w) > 4: w = w[:-3]
            elif w.endswith('—è—Ç—å') and len(w) > 4: w = w[:-3]
            elif w.endswith('–∏—Ç—å') and len(w) > 4: w = w[:-3]
            elif w.endswith('–µ—Ç—å') and len(w) > 4: w = w[:-3]
            elif w.endswith('—ã–π') or w.endswith('–∏–π') or w.endswith('–æ–π'): w = w[:-2]
            elif w.endswith('–∞—è') or w.endswith('—è—è'): w = w[:-2]
            elif w.endswith('–æ–µ') or w.endswith('–µ–µ'): w = w[:-2]
            elif w.endswith('–∞–º') or w.endswith('—è–º'): w = w[:-2]
            elif w.endswith('–∞–º–∏') or w.endswith('—è–º–∏'): w = w[:-3]
            elif w.endswith('–∞—Ö') or w.endswith('—è—Ö'): w = w[:-2]
            elif w.endswith('–æ–≤') or w.endswith('–µ–≤'): w = w[:-2]
            elif w.endswith('–µ–π'): w = w[:-2]
            normalized.append(w)
        
        return ' '.join(normalized)

    # ------------------------------------------------------------
    #  –ë–´–°–¢–†–ê–Ø –ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø (–ë–ï–ó –õ–ï–í–ï–ù–®–¢–ï–ô–ù–ê)
    # ------------------------------------------------------------
    def _quick_match(self, norm_query: str, faq: FAQEntry) -> bool:
        """
        –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å—Ç—å –ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å–ª–æ–≤ –∑–∞–ø—Ä–æ—Å–∞
        —Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º –≤–æ–ø—Ä–æ—Å–æ–º –∏–ª–∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏.
        """
        if not norm_query:
            return False
        q_words = set(norm_query.split())
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è —Å –≤–æ–ø—Ä–æ—Å–æ–º
        if faq.norm_question:
            q_words_question = set(faq.norm_question.split())
            if q_words.intersection(q_words_question):
                return True
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        if faq.norm_keywords:
            q_words_keywords = set(faq.norm_keywords.split())
            if q_words.intersection(q_words_keywords):
                return True
        return False

    # ------------------------------------------------------------
    #  –ü–û–õ–ù–´–ô –†–ê–°–ß–Å–¢ –†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–ò (–° –õ–ï–í–ï–ù–®–¢–ï–ô–ù–û–ú)
    # ------------------------------------------------------------
    def _calculate_full_score(self, norm_query: str, query_words: set, faq: FAQEntry) -> float:
        """–ü–æ–ª–Ω—ã–π —Ä–∞—Å—á—ë—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞."""
        score = 0.0

        # 1. –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
        if norm_query == faq.norm_question:
            return 100.0

        # 2. –ó–∞–ø—Ä–æ—Å —è–≤–ª—è–µ—Ç—Å—è –ø–æ–¥—Å—Ç—Ä–æ–∫–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
        if norm_query in faq.norm_question:
            score += 50.0

        # 3. –ù–µ—á—ë—Ç–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ (–õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω)
        if len(norm_query) >= 4 and faq.norm_question:
            lev_dist = levenshtein_distance(norm_query, faq.norm_question)
            if lev_dist == 0:
                return 100.0
            elif lev_dist <= 2:
                score += 40.0
            elif lev_dist <= 4:
                score += 20.0
            if faq.norm_keywords:
                kw_lev = levenshtein_distance(norm_query, faq.norm_keywords[:len(norm_query)+5])
                if kw_lev <= 2:
                    score += 30.0

        # 4. –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ —Å–ª–æ–≤–∞–º –≤ –≤–æ–ø—Ä–æ—Å–µ
        q_words = set(faq.norm_question.split()) if faq.norm_question else set()
        common_q = query_words.intersection(q_words)
        score += len(common_q) * 12.0

        # 5. –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        if faq.norm_keywords:
            kw_words = set(faq.norm_keywords.split())
            common_kw = query_words.intersection(kw_words)
            score += len(common_kw) * 20.0

        # 6. –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤
        for word in query_words:
            if len(word) > 3:
                if word in faq.norm_question:
                    score += 3.0
                if faq.norm_keywords and word in faq.norm_keywords:
                    score += 5.0

        return score

    # ------------------------------------------------------------
    #  –û–°–ù–û–í–ù–û–ô –ü–û–ò–°–ö (–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô)
    # ------------------------------------------------------------
    def search(self, query: str, category: Optional[str] = None, top_k: int = 5) -> List[Tuple[str, str, float]]:
        if not query or len(query.strip()) < 2:
            return []
        if not self.faq_data:
            logger.warning("‚ö†Ô∏è –ü–æ–∏—Å–∫ –ø—Ä–∏ –ø—É—Å—Ç–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π")
            return []
        
        norm_query = self._normalize_text(query)
        if not norm_query:
            return []
        
        cache_key = f"{norm_query}_{category}_{top_k}"
        cache_key_hash = hashlib.md5(cache_key.encode()).hexdigest()[:16]
        
        if cache_key_hash in self.cache:
            expiry = self.cache_ttl.get(cache_key_hash)
            if expiry and datetime.now() < expiry:
                self.stats['cache_hits'] += 1
                self.stats['total_searches'] += 1
                self.cache.move_to_end(cache_key_hash)
                return self.cache[cache_key_hash]
        
        self.stats['total_searches'] += 1
        self.stats['cache_misses'] += 1
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–Ω–µ—Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)
        if category:
            cat_lower = category.lower()
            faq_list = self._category_index.get(cat_lower, [])
            if not faq_list:
                for cat_key, entries in self._category_index.items():
                    if cat_lower in cat_key:
                        faq_list = entries
                        logger.debug(f"–ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: '{category}' -> '{cat_key}'")
                        break
        else:
            faq_list = self.faq_data
        
        if not faq_list:
            return []
        
        query_words = set(norm_query.split())
        
        # --- –≠–¢–ê–ü 1: –ë–´–°–¢–†–ê–Ø –ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø ---
        preliminary = []
        for faq in faq_list:
            if self._quick_match(norm_query, faq):
                preliminary.append(faq)
        
        # –ï—Å–ª–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–µ 20 –∏–∑ faq_list
        if not preliminary:
            preliminary = faq_list[:20]
        
        # --- –≠–¢–ê–ü 2: –ü–û–õ–ù–´–ô –†–ê–°–ß–Å–¢ –î–õ–Ø –¢–û–ü-10 –ö–ê–ù–î–ò–î–ê–¢–û–í ---
        # –°–Ω–∞—á–∞–ª–∞ –Ω–∞–±–µ—Ä—ë–º –±–∞–∑–æ–≤—ã–µ –æ—á–∫–∏ –±–µ–∑ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ (—Ç–æ–ª—å–∫–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å–ª–æ–≤)
        candidates_with_score = []
        for faq in preliminary[:20]:  # –ª–∏–º–∏—Ç 20 –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
            # –±—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–æ–ª—å–∫–æ –ø–æ —Å–ª–æ–≤–∞–º (–±–µ–∑ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞)
            base_score = 0.0
            q_words = set(faq.norm_question.split()) if faq.norm_question else set()
            common_q = query_words.intersection(q_words)
            base_score += len(common_q) * 12.0
            if faq.norm_keywords:
                kw_words = set(faq.norm_keywords.split())
                common_kw = query_words.intersection(kw_words)
                base_score += len(common_kw) * 20.0
            candidates_with_score.append((faq, base_score))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –±–∞–∑–æ–≤–æ–π –æ—Ü–µ–Ω–∫–µ –∏ –±–µ—Ä—ë–º —Ç–æ–ø-10
        candidates_with_score.sort(key=lambda x: x[1], reverse=True)
        top_candidates = [faq for faq, _ in candidates_with_score[:10]]
        
        # –¢–µ–ø–µ—Ä—å –≤—ã—á–∏—Å–ª—è–µ–º –ø–æ–ª–Ω—É—é –æ—Ü–µ–Ω–∫—É (—Å –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–æ–º) –¥–ª—è —Ç–æ–ø-10
        results = []
        for faq in top_candidates:
            score = self._calculate_full_score(norm_query, query_words, faq)
            if score > 0:
                results.append((faq.question, faq.answer, min(score, 100.0)))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏—Ç–æ–≥–æ–≤–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        results.sort(key=lambda x: x[2], reverse=True)
        top_results = results[:top_k]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        if top_results:
            if len(self.cache) >= self.max_cache_size:
                oldest = next(iter(self.cache))
                del self.cache[oldest]
                del self.cache_ttl[oldest]
            self.cache[cache_key_hash] = top_results
            self.cache_ttl[cache_key_hash] = datetime.now() + timedelta(hours=1)
        
        return top_results

    # ------------------------------------------------------------
    #  –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø –ü–û –ò–°–ü–†–ê–í–õ–ï–ù–ò–Æ –ó–ê–ü–†–û–°–ê
    # ------------------------------------------------------------
    def suggest_correction(self, query: str, top_k: int = 3) -> List[str]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤, –Ω–∞–∏–±–æ–ª–µ–µ –±–ª–∏–∑–∫–∏—Ö –∫ –∑–∞–ø—Ä–æ—Å—É –ø–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –∫–æ–≥–¥–∞ –ø–æ–∏—Å–∫ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
        """
        if not query or not self.faq_data:
            return []
        
        norm_query = self._normalize_text(query)
        if not norm_query or len(norm_query) < 3:
            return []
        
        candidates = []
        for faq in self.faq_data[:50]:  # –æ–≥—Ä–∞–Ω–∏—á–∏–º –ø–µ—Ä–≤—ã–º–∏ 50 –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            if faq.norm_question:
                dist = levenshtein_distance(norm_query, faq.norm_question)
                if dist <= 5:  # —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–ª–∏–∑–∫–∏–µ
                    candidates.append((faq.question, dist))
        
        candidates.sort(key=lambda x: x[1])
        return [q for q, _ in candidates[:top_k]]

    # ------------------------------------------------------------
    #  –£–ü–†–ê–í–õ–ï–ù–ò–ï –ò –°–¢–ê–¢–ò–°–¢–ò–ö–ê
    # ------------------------------------------------------------
    def refresh_data(self):
        self._load_faq()
        self._build_category_index()
        self.cache.clear()
        self.cache_ttl.clear()
        logger.info("üîÑ –î–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–µ–Ω—ã, –∫—ç—à —Å–±—Ä–æ—à–µ–Ω")

    def get_stats(self) -> Dict[str, Any]:
        categories = {}
        for faq in self.faq_data:
            categories[faq.category] = categories.get(faq.category, 0) + 1
        
        cache_hit_rate = 0.0
        if self.stats['total_searches'] > 0:
            cache_hit_rate = (self.stats['cache_hits'] / self.stats['total_searches']) * 100
        
        return {
            'faq_count': len(self.faq_data),
            'categories': len(categories),
            'category_list': sorted(categories.keys()),
            'category_counts': categories,
            'cache_size': len(self.cache),
            'max_cache_size': self.max_cache_size,
            'total_searches': self.stats['total_searches'],
            'cache_hits': self.stats['cache_hits'],
            'cache_misses': self.stats['cache_misses'],
            'cache_hit_rate': round(cache_hit_rate, 2),
            'loaded_from': self.stats['loaded_from']
        }

    def get_faq_by_id(self, faq_id: int) -> Optional[Dict]:
        for faq in self.faq_data:
            if faq.id == faq_id:
                return {
                    'id': faq.id,
                    'question': faq.question,
                    'answer': faq.answer,
                    'category': faq.category,
                    'keywords': faq.keywords
                }
        return None

# –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
EnhancedSearchEngine = SearchEngine
